---
layout: layout
title: "Research Overview"
---

<center>   
  "By seeking and blundering we learn." â€“ Johann Wolfgang von Goethe<br>
  Few of my learnings: <a href='#preprints'>preprints</a> or <a href='#publications'> selected publications.</a>
</center>

<h2 id='preprints' class="page-heading">Preprints</h1>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/selection_strategy_mini.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/pdf/2408.14470">ArXiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Step-by-Step Unmasking for PEFT in LLMs </b>
      <p> Aradhye Agarwal*, <b>Suhas Kamasetty Ramesh*</b>, Ayan Sengupta*, Tanmoy Chakraborty</p><br>
      <p>Traditional Selective PEFT methods choose parameters for fine-tuning during the initial phase of training.
         These methods fail to capture the change in parameter importance during later phases of training.
         We introduce an incremental strategy with an appropriate heuristic to solve these problems while also reducing the number of parameter updates.
      </p>
    </div>
  </div>

<div class="divider"></div>

<h2 id='publications' class="page-heading"> Selected Publications</h1>

  

  <div class="divider"></div>

  <div class="divider"></div>

<!-- 
<!--   <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/nam.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://neural-additive-models.github.io/">Website</a></td>
          <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
          <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
        </tr>
      </table>
    </div> -->

<!--     <div class="six columns">

      <b> Neural Additive Models: Interpretable ML with Neural Nets</b>
      <p> <b>Rishabh Agarwal</b>, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, Geoffrey Hinton<br />
      NeurIPS 2021 (<span style='color:red'>Spotlight</span>) </p>
      </p>
    <p> We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility 
      of generalized additive models. NAMs are likely to be combined with deep learning methods in ways we don't foresee. 
      This is important because one of the key drawbacks of deep learning is intelligibility. 
    </p>
    </div>  -->
<!--   </div>

  <div class="divider"></div>
 -->

<!-- <div class="divider"></div> -->

<!--  <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/replay.jpg">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/2007.06700">ArXiv</a></td>
        <td><a href="https://github.com/google-research/google-research/tree/master/experience_replay">Code</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> Revisiting Fundamentals of Experience Replay</b>
      <p> William Fedus, Prajit Ramachandran, <b>Rishabh Agarwal</b>, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will Dabney <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. 
      We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: 
      the replay capacity and the ratio of learning updates to experience collected (replay ratio).
    </p>
    </div>
 </div> -->

<!-- <div class="divider"></div> -->

<!-- <div class="row">
<!--   <div class="six columns">
<img style="margin-top:0em" src="/images/research/merl.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.07198">ArXiv</a></td>
    <td><a href="https://agarwl.github.io/merl/">Website</a></td>
    <td><a href="https://www.youtube.com/watch?v=_IXj6kXPdq8&feature=youtu.be">Talk</a></td>
    <!-- <td><a href="https://github.com/google-research/google-research/tree/master/meta_reward_learning">Code</a></td> -->
<!--   </tr>
</table>
  </div> --> 

<!--   <div class="six columns">

    <b> Learning to Generalize from Sparse and Underspecified Rewards</b>
    <p><b>Rishabh Agarwal</b>, Chen Liang, Dale Schuurmans, Mohammad Norouzi <br />
    ICML 2019 (<span style='color:red'>Short Talk</span>).
  </p>
 <!-- <p>Reinforcement learning (RL) has enabled remarkable success in addressing challenging tasks such as playing games such as Atari and Go, continuous control, and robotic learning. -->
<!--   <p> Many real-world problems involve sparse and underspecified rewards, requiring a learning agent to generalize from limited feedback. In this work, we propose an effective exploration strategy for tackling sparse rewards and Meta Reward Learning to deal with underspecified rewards. These approaches provide substantial gains in language understanding tasks such as instruction following and semantic parsing.</p>
    </p> -->
<!--   </div> -->
<!-- </div> --> 





</div>
