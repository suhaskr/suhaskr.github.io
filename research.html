---
layout: layout
title: "Research Overview"
---

<center>   
  "By seeking and blundering we learn." â€“ Johann Wolfgang von Goethe<br>
  <!-- Few of my learnings: <a href='#preprints'>preprints</a> or <a href='#publications'> selected publications.</a> -->
</center>

<h2 id='preprints' class="page-heading">Preprints</h1>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/selection_strategy_mini.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/pdf/2408.14470">ArXiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> <font size="+2">Step-by-Step Unmasking for PEFT in LLMs </font> </b>
      <p> Aradhye Agarwal*, <b>Suhas Kamasetty Ramesh*</b>, Ayan Sengupta*, Tanmoy Chakraborty</p><br>
      <p>Traditional Selective PEFT methods choose parameters for fine-tuning during the initial phase of training.
         These methods fail to capture the change in parameter importance during later phases of training.
         We introduce an incremental strategy with an appropriate heuristic to solve these problems while also reducing the number of parameter updates.
      </p>
    </div>
  </div>

<div class="divider"></div>

<h2 id='publications' class="page-heading"> Selected Publications</h1>
<div class="row">
  <div class="five columns">
    <p> . </p>
  </div>
</div>
<div class="row">
  <div class="seven columns">
    <p>Coming soon</p>
  </div>
</div>
  

  <div class="divider"></div>

  <div class="divider"></div>

<!-- 
<!--   <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/nam.jpeg" width="150">
      <table>
        <tr>          
          <td><a href="http://neural-additive-models.github.io/">Website</a></td>
          <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
          <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
        </tr>
      </table>
    </div> -->

<!--   </div>

  <div class="divider"></div>
 -->

<!-- <div class="divider"></div> -->

<!-- <div class="divider"></div> -->

<!-- <div class="row">
<!--   <div class="six columns">
<img style="margin-top:0em" src="/images/research/merl.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.07198">ArXiv</a></td>
    <td><a href="https://agarwl.github.io/merl/">Website</a></td>
    <td><a href="https://www.youtube.com/watch?v=_IXj6kXPdq8&feature=youtu.be">Talk</a></td>
    <!-- <td><a href="https://github.com/google-research/google-research/tree/master/meta_reward_learning">Code</a></td> -->
<!--   </tr>
</table>
  </div> --> 

<!--   <div class="six columns">

 <!-- <p>Reinforcement learning (RL) has enabled remarkable success in addressing challenging tasks such as playing games such as Atari and Go, continuous control, and robotic learning. -->
<!--   <p> Many real-world problems involve sparse and underspecified rewards, requiring a learning agent to generalize from limited feedback. In this work, we propose an effective exploration strategy for tackling sparse rewards and Meta Reward Learning to deal with underspecified rewards. These approaches provide substantial gains in language understanding tasks such as instruction following and semantic parsing.</p>
    </p> -->
<!--   </div> -->
<!-- </div> --> 





</div>
